# Quick Reference Card

## Installation (One-time)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
python verify_setup.py
```

## Basic Workflow

```bash
# 1. Place data
cp your_data.mat data/

# 2. Run everything
make all

# Or run step-by-step:
make baselines   # Cubic, polyphase, FFT â†’ ~5s
make ou          # OU-Kalman â†’ ~10s
make train       # Train diffusion model â†’ ~10min (CPU) / ~2min (GPU)
make sample      # Generate with PG-CDI â†’ ~2min
make eval        # Evaluate all methods â†’ ~5s
```

## Output Files

```
out/
â”œâ”€â”€ 1000fps_cubic_nm.csv        # Cubic spline
â”œâ”€â”€ 1000fps_polyphase_nm.csv    # Polyphase resampling
â”œâ”€â”€ 1000fps_fft_nm.csv          # FFT resampling
â”œâ”€â”€ 1000fps_ou_interp_nm.csv    # OU continuous interpolation
â”œâ”€â”€ 1000fps_ou_fullgrid_nm.csv  # OU full state-space (best classical)
â”œâ”€â”€ 1000fps_pgcdi_nm.csv        # PG-CDI (novel method)
â”œâ”€â”€ overlay_cubic_fft.png       # Visual comparison
â”œâ”€â”€ ou_fullgrid_uncertainty.png # Confidence bands
â”œâ”€â”€ eval_summary.json           # Quantitative metrics
â””â”€â”€ pgcdi.pt                    # Trained model
```

## Configuration

Edit `src/obt1000/config.py`:

```python
# Essential settings
mat_path = "data/your_file.mat"    # Input file
fps_in = 300.0                     # Input frame rate
fps_out = 1000.0                   # Target frame rate
px_to_nm = 35.0                    # Calibration factor

# Training (PG-CDI)
epochs = 20                        # Training epochs
batch_size = 4                     # Batch size
lr = 2e-4                          # Learning rate
```

## Methods Summary

| Method | Type | Speed | Uncertainty | Physics-aware |
|--------|------|-------|-------------|---------------|
| Cubic | Classical | âš¡âš¡âš¡ | âŒ | âŒ |
| Polyphase | Classical | âš¡âš¡âš¡ | âŒ | âŒ |
| FFT | Classical | âš¡âš¡âš¡ | âŒ | âŒ |
| OU Interp | Physics | âš¡âš¡ | âŒ | âœ… |
| OU Fullgrid | Physics | âš¡âš¡ | âœ… | âœ… |
| PG-CDI | ML+Physics | âš¡ | âŒ | âœ… |

## Commands

```bash
# Verify setup
python verify_setup.py

# Individual scripts
python scripts/run_baselines.py
python scripts/run_ou.py
python scripts/run_train_pgcdi.py
python scripts/run_sample_pgcdi.py
python scripts/run_eval_all.py

# Clean outputs
rm -rf out/*

# Re-train model
make train

# View results
cat out/eval_summary.json
open out/overlay_cubic_fft.png
open out/ou_fullgrid_uncertainty.png
```

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Import errors | `pip install -e .` |
| No .mat file | Check `data/` directory |
| Wrong variable | Set `mat_key` in config |
| Out of memory | Reduce `batch_size` or `T` |
| Slow training | Use GPU or reduce `epochs` |
| Bad results | Check `px_to_nm` calibration |

## Quick Tests

```bash
# Test imports
python -c "from src.obt1000.config import cfg; print(cfg)"

# Test data loading (requires .mat file)
python -c "from src.obt1000.io_prep import load_mat_as_nm; print(load_mat_as_nm())"

# Check GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

## Typical Performance (15k points @ 300 fps â†’ 50k @ 1000 fps)

| Step | CPU Time | GPU Time | Output |
|------|----------|----------|--------|
| Baselines | 5s | N/A | 3 CSV files |
| OU-Kalman | 10s | N/A | 2 CSV files |
| Training | 10-20min | 1-3min | 1 model file |
| Sampling | 2-5min | 30-60s | 1 CSV file |
| Evaluation | 5s | N/A | 1 JSON file |

## Key Equations

**OU Process:**
```
dx/dt = -(x/Ï„) + âˆš(2D) * Î¾(t)
AR(1): x[n+1] = a*x[n] + w[n], a = exp(-Î”t/Ï„)
```

**Kalman Filter:**
```
Predict: xÌ‚[k|k-1] = a*xÌ‚[k-1]
Update:  xÌ‚[k] = xÌ‚[k|k-1] + K*(y[k] - xÌ‚[k|k-1])
```

**Diffusion:**
```
Forward:  x_t = âˆš(á¾±_t)*x_0 + âˆš(1-á¾±_t)*Îµ
Reverse:  x_{t-1} = (x_t - Î²_t*Îµ_Î¸)/âˆš(1-Î²_t)
```

## File Structure

```
.
â”œâ”€â”€ README.md                    # Main readme
â”œâ”€â”€ USAGE_GUIDE.md              # Detailed usage
â”œâ”€â”€ CONFIGURATION_EXAMPLES.md    # Config examples
â”œâ”€â”€ QUICK_REFERENCE.md          # This file
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ pyproject.toml              # Package config
â”œâ”€â”€ Makefile                    # Build commands
â”œâ”€â”€ verify_setup.py             # Verification script
â”œâ”€â”€ data/                       # Input data (your .mat)
â”œâ”€â”€ out/                        # Output results
â”œâ”€â”€ scripts/                    # Runnable scripts
â”‚   â”œâ”€â”€ run_baselines.py
â”‚   â”œâ”€â”€ run_ou.py
â”‚   â”œâ”€â”€ run_train_pgcdi.py
â”‚   â”œâ”€â”€ run_sample_pgcdi.py
â”‚   â””â”€â”€ run_eval_all.py
â””â”€â”€ src/
    â”œâ”€â”€ obt1000/               # Classical methods
    â”‚   â”œâ”€â”€ config.py          # Configuration
    â”‚   â”œâ”€â”€ io_prep.py         # Data I/O
    â”‚   â”œâ”€â”€ baselines.py       # Interpolation
    â”‚   â”œâ”€â”€ ou_kalman.py       # OU filtering
    â”‚   â”œâ”€â”€ eval_metrics.py    # Evaluation
    â”‚   â”œâ”€â”€ plotting.py        # Visualization
    â”‚   â””â”€â”€ utils.py           # Utilities
    â””â”€â”€ pgcdi/                 # Novel ML method
        â”œâ”€â”€ data.py            # Synthetic data
        â”œâ”€â”€ model.py           # UNet architecture
        â”œâ”€â”€ physics_losses.py  # Physics constraints
        â”œâ”€â”€ scheduler.py       # Noise schedule
        â”œâ”€â”€ train.py           # Training loop
        â””â”€â”€ sample.py          # Inference
```

## Citation

```bibtex
@software{optical_bead_1000fps,
  title = {Physics-Guided Upsampling of Optical Tweezer Trajectories},
  year = {2025},
  url = {https://github.com/yourusername/optical-bead-trajectory-1000fps}
}
```

## Support

1. âœ… Run `python verify_setup.py`
2. ğŸ“– Read `USAGE_GUIDE.md`
3. ğŸ”§ Check `CONFIGURATION_EXAMPLES.md`
4. ğŸ› Open GitHub issue
